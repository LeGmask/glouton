#
#  Copyright 2015-2016 Bleemeo
#
#  bleemeo.com an infrastructure monitoring solution in the Cloud
#
#   Licensed under the Apache License, Version 2.0 (the "License");
#   you may not use this file except in compliance with the License.
#   You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

import distutils.version
import logging
import os
import re
import shlex
import subprocess
import time

import psutil
import requests
from six.moves import urllib_parse

import bleemeo_agent.util


BASE_TELEGRAF_CONFIG = """# Configuration generated by Bleemeo-agent.
# do NOT modify, it will be overwrite on next agent start.
"""

STATSD_TELEGRAF_CONFIG = """
# Statsd Server
# To disable Statsd server, add:
#     telegraf:
#         statsd:
#             enabled: False
# in /etc/bleemeo/agent.conf.d/99-local.conf
[[inputs.statsd]]
  service_address = "%(address)s:%(port)s"
  delete_gauges = false
  delete_counters = false
  delete_timings = true
  percentiles = [90]
  metric_separator = "_"
  allowed_pending_messages = 10000
  percentile_limit = 1000
"""

APACHE_TELEGRAF_CONFIG = """
[[inputs.apache]]
  urls = ["%(status_url)s"]
"""

# Additional configuration if Telegraf >= 1.2.0
APACHE_TELEGRAF_CONFIG_1_2 = """
  insecure_skip_verify = true
"""


DOCKER_TELEGRAF_CONFIG = """
[[inputs.docker]]
    perdevice = false
    total = true
"""

ELASTICSEARCH_TELEGRAF_CONFIG = """
[[inputs.elasticsearch]]
  servers = ["http://%(address)s:%(port)s"]
  local = true
  cluster_health = false
"""

HAPROXY_TELEGRAF_CONFIG = """
[[inputs.haproxy]]
  servers = ["%(stats_url)s"]
"""

MEMCACHED_TELEGRAF_CONFIG = """
[[inputs.memcached]]
  servers = ["%(address)s:%(port)s"]
"""

MONGODB_TELEGRAF_CONFIG = """
[[inputs.mongodb]]
  servers = ["%(address)s:%(port)s"]
"""

MYSQL_TELEGRAF_CONFIG = """
[[inputs.mysql]]
  servers = ["%(username)s:%(password)s@tcp(%(address)s:%(port)s)/"]
"""

# Additional configuration if Telegraf >= 1.3.0
MYSQL_TELEGRAF_CONFIG_1_3 = """
  gather_innodb_metrics = true
"""

NGINX_TELEGRAF_CONFIG = """
[[inputs.nginx]]
  urls = ["http://%(address)s:%(port)s/nginx_status"]
"""

# Additional configuration if Telegraf >= 1.4.0
NGINX_TELEGRAF_CONFIG_1_4 = """
  insecure_skip_verify = true
"""

PHPFPM_TELEGRAF_CONFIG = """
[[inputs.phpfpm]]
    urls = ["%(stats_url)s"]
    %(tags)s
"""  # noqa

POSTGRESQL_TELEGRAF_CONFIG = """
[[inputs.postgresql]]
    address = "host=%(address)s port=%(port)s user=%(username)s password=%(password)s dbname=postgres sslmode=disable"
"""  # noqa

PROMETHEUS_TELEGRAF_CONFIG = """
[[inputs.prometheus]]
  urls = ["%(url)s"]
  name_prefix = "prometheus_%(prefix)s_"
"""

RABBITMQ_TELEGRAF_CONFIG = """
[[inputs.rabbitmq]]
  url = "http://%(address)s:%(mgmt_port)s"
  username = "%(username)s"
  password = "%(password)s"
"""

REDIS_TELEGRAF_CONFIG = """
[[inputs.redis]]
  servers = ["tcp://%(address)s:%(port)s"]
"""

ZOOKEEPER_CONFIG = """
[[inputs.zookeeper]]
  servers = ["%(address)s:%(port)s"]
"""


class ComputationFail(Exception):
    pass


class MissingMetric(Exception):
    pass


def compare_version(current_version, wanted_version):
    """ Return True if current_version is greater or equal to wanted_version
    """
    current_version = distutils.version.LooseVersion(current_version)
    wanted_version = distutils.version.LooseVersion(wanted_version)
    return current_version >= wanted_version


def services_sorted(services_items):
    """ Sort core.services.items() result

        Result is sorted by service name, then by instances
    """
    def sort_key(item):
        """ Return a comparable couple (service_name, instance_name)

            This is needed because instance could be None, and None is
            not comparable to a string in Python 3
        """
        ((service_name, instance), _) = item

        instance_name = '' if instance is None else instance
        return (service_name, instance_name)

    return sorted(services_items, key=sort_key)


def telegraf_replace(value):
    """ telegraf replace some char before sending to graphite.

        This function do the same transformation
    """
    return (
        value
        .replace('/', '-')
        .replace('@', '-')
        .replace('*', '-')
        .replace(' ', '_')
        .replace('..', '.')
        .replace('\\', '')
        .replace(')', '_')
        .replace('(', '_')
        .replace('.', '_')
    )


def update_discovery(core):
    try:
        _write_config(core)
    except Exception:
        logging.warning(
            'Failed to write telegraf configuration. '
            'Continuing with current configuration')
        logging.debug('exception is:', exc_info=True)


class Telegraf:

    def __init__(self, graphite_client):
        self.core = graphite_client.core
        self.graphite_client = graphite_client
        self.graphite_server = graphite_client.server

        # used to compute derivated values
        self._raw_value = {}

        self.computed_metrics_pending = set()

        self.last_timestamp = 0

        self.core.add_scheduled_job(
            self._purge_metrics,
            seconds=5 * 60,
        )

    def _purge_metrics(self):
        """ Remove old metrics from self._raw_value
        """
        now = time.time()
        cutoff = now - 60 * 6

        # XXX: concurrent access with emit_metric
        self._raw_value = {
            key: (timestamp, value)
            for key, (timestamp, value) in self._raw_value.items()
            if timestamp >= cutoff
        }

    def get_derivate(self, name, item, timestamp, value):
        """ Return derivate of a COUNTER (e.g. something that only goes upward)
        """
        (old_timestamp, old_value) = self._raw_value.get(
            (name, item), (None, None)
        )
        self._raw_value[(name, item)] = (timestamp, value)
        if old_timestamp is None:
            return None

        delta = value - old_value
        delta_time = timestamp - old_timestamp

        if delta_time == 0:
            return None

        if delta < 0:
            return None

        return delta / delta_time

    def get_service_instance(self, service, address, port):
        for (key, service_info) in self.core.services.items():
            (service_name, instance) = key
            if (service_name == service
                    and service_info.get('address') == address
                    and service_info.get('port') == port):
                return instance
            # RabbitMQ use mgmt port
            if (service_name == service
                    and service_name == 'rabbitmq'
                    and service_info.get('address') == address
                    and service_info.get('mgmt_port', 15672) == port):
                return instance

        raise KeyError('service not found')

    def get_haproxy_instance(self, hostport):
        if ':' in hostport:
            host, port = hostport.split(':')
            port = int(port)
        else:
            host = hostport
            port = None

        for (key, service_info) in self.core.services.items():
            (service_name, instance) = key
            if service_name != 'haproxy':
                continue
            if 'stats_url' in service_info:
                tmp = urllib_parse.urlparse(service_info['stats_url'])
                if host == tmp.hostname and port == tmp.port:
                    return instance

        raise KeyError('service not found')

    def get_elasticsearch_instance(self, node_id):
        for (key, service_info) in self.core.services.items():
            (service_name, instance) = key
            if service_name != 'elasticsearch':
                continue
            if not service_info.get('active', True):
                continue
            if service_info.get('address') is None:
                continue
            if service_info.get('port') is None:
                continue
            if 'es_node_id' not in service_info:
                try:
                    response = requests.get(
                        'http://%(address)s:%(port)s/_nodes/_local/'
                        % service_info,
                        headers={'User-Agent': self.core.http_user_agent},
                        timeout=10.0,
                    )
                    data = response.json()
                    this_node_id = list(data['nodes'].keys())[0]
                except (requests.RequestException, ValueError):
                    logging.debug(
                        'Error while fetching es_node_is', exc_info=True
                    )
                    continue

                service_info['es_node_id'] = this_node_id

            if service_info.get('es_node_id') == node_id:
                return instance

        raise KeyError('service not found')

    def get_prometheus_exporter_name(self, metric_name, part):
        """ Return the config of the Prometheus exporter
        """
        prometheus_configs = self.core.config.get('metric.prometheus', {})
        for name, config in prometheus_configs.items():
            url_mangled = telegraf_replace(config['url'])
            if (metric_name.startswith('%s_' % name)
                    and url_mangled in part):
                return name
        raise KeyError('prometheus config not found')

    def docker_container_name(self, part):
        """ Return Docker container name for given graphite line.

            This method does two thing:

            1) Find where the container_name is stored in graphite line
            2) Find the real name of container_name

            For the 2, when sent over graphite protocol, Telegraf replaces
            some char from container_name to "_". This method search which
            container name match the mangled name.

            For the 1, the position on the graphite line of container_name is
            not the same because Telegraf sent all container labels. This
            method find where container_name is based on defined labels for
            each container.

            A container without any labels would only have the following tags:

            host=xenial,container_image=redis,container_name=labeled_redis,
                container_version=unknown,engine_host=docker-host

            Or with older Telegraf (< 1.1.0):

            host=xenial,container_image=redis,container_name=labeled_redis,
                container_version=unknown

            That result in graphite line:

            xenial.redis.labeled_redis.unknown.docker-host

            The version change didn't impact use, as the added
            tag "engine_host", is always after "container_name".

            Here container_name is the 3th position. But if user add a label
            "a_custom_label=my_value", then the graphite line result in:

            xenial.a_custom_label.redis.labeled_redis.unknown

            The container_name is now 4th position.

            In general case, the container_name position is 3 + number of
            label keys that are (in lexical order) before "container_name".
        """

        for container_name, inspect in self.core.docker_containers.items():
            labels = inspect.get('Config', {}).get('Labels', {})
            if labels is None:
                labels = {}
            label_keys_before = [
                key for (key, value) in labels.items()
                if key < 'container_name' and value != ''
            ]
            position = 3 + len(label_keys_before)

            # Docker only allow "_", "." and "-" as special char in
            # container_name. Of those, only "." is replaced by "_"
            tmp = container_name.replace('.', '_')
            if len(part) > position and part[position] == tmp:
                return container_name

        return None

    def close(self):
        self._check_computed_metrics()

    def emit_metric(self, name, timestamp, value):
        self.graphite_server.data_last_seen_at = bleemeo_agent.util.get_clock()

        if timestamp - self.last_timestamp > 1:
            self._check_computed_metrics()
        self.last_timestamp = timestamp

        item = None
        service = None
        instance = None
        container_name = None
        derive = False
        no_emit = False

        # name looks like
        # telegraf.HOSTNAME.(ITEM_INFO)*.PLUGIN.METRIC
        # example:
        # telegraf.xps-pierref.ext4./home.disk.total
        # telegraf.xps-pierref.mem.used
        # telegraf.xps-pierref.cpu-total.cpu.usage_steal
        part = name.split('.')

        if part[-2] == 'cpu':
            if part[-3] != 'cpu-total':
                return

            name = part[-1].replace('usage_', 'cpu_')
            if name == 'cpu_irq':
                name = 'cpu_interrupt'
            elif name == 'cpu_iowait':
                name = 'cpu_wait'

            if name == 'cpu_idle':
                self.core.emit_metric({
                    'measurement': 'cpu_used',
                    'time': timestamp,
                    'value': 100 - value,
                })
            self.computed_metrics_pending.add(
                ('cpu_other', None, None, timestamp)
            )
        elif part[-2] == 'win_cpu':
            if part[2] != '_Total':
                return

            name = part[-1]
            if name == 'Percent_Idle_Time':
                name = 'cpu_idle'
            elif name == 'Percent_Interrupt_Time':
                name = 'cpu_interrupt'
            elif name == 'Percent_User_Time':
                name = 'cpu_user'
            elif name == 'Percent_Privileged_Time':
                name = 'cpu_system'
            elif name == 'Percent_DPC_Time':
                name = 'cpu_softirq'
            else:
                return

            if name == 'cpu_idle':
                self.core.emit_metric({
                    'measurement': 'cpu_used',
                    'time': timestamp,
                    'value': 100 - value,
                })
                self.computed_metrics_pending.add(
                    ('system_load1', None, None, timestamp)
                )
            self.computed_metrics_pending.add(
                ('cpu_other', None, None, timestamp)
            )
        elif part[-2] == 'disk':
            path = part[-3].replace('-', '/')
            path = self.graphite_server.disk_path_rename(path)
            if path is None:
                return
            item = path

            name = 'disk_' + part[-1]
            if name == 'disk_used_percent':
                name = 'disk_used_perc'
        elif part[-2] == 'win_disk':
            item = part[2]
            name = part[-1]
            if item == '_Total':
                return

            # For Windows, assimilate disk (which are also named "C:", "D:"...
            # and (mounted) partition like C:
            if self.graphite_server._ignored_disk(item):
                return

            if name == 'Percent_Free_Space':
                name = 'disk_used_perc'
                value = 100 - value

                # when disk_total is processed, disk_used is also emitted
                self.computed_metrics_pending.add(
                    ('disk_total', item, None, timestamp)
                )
            elif name == 'Free_Megabytes':
                name = 'disk_free'
                value = value * 1024 * 1024
                self.computed_metrics_pending.add(
                    ('disk_total', item, None, timestamp)
                )
            else:
                return
        elif part[-2] == 'diskio':
            item = part[2]
            name = part[-1]
            if not name.startswith('io_'):
                name = 'io_' + name
            if self.graphite_server._ignored_disk(item):
                return
            if name == 'io_weighted_io_time':
                return

            if name == 'io_iops_in_progress':
                name = 'io_in_progress'
            else:
                value = self.get_derivate(name, item, timestamp, value)
                if value is None:
                    return

            if name == 'io_time':
                self.core.emit_metric({
                    'measurement': 'io_utilization',
                    # io_time is a number of ms spent doing IO (per seconds)
                    # utilization is 100% when we spent 1000ms during one
                    # second
                    'value': value / 1000. * 100.,
                    'time': timestamp,
                    'item': item,
                })
        elif part[-2] == 'win_diskio':
            item = part[2]
            name = part[-1]
            if item == '_Total':
                return

            # Item looks like "0_C:", "1_D:" or "0_C:_D:" (multiple partition
            # on one disk). Remove the number_ from item and take the smaller
            # letter.
            if '_' in item:
                item_part = item.split('_')
                number = item_part[0]
                try:
                    int(number)
                    item = sorted(item_part[1:])[0]
                except ValueError:
                    pass

            if self.graphite_server._ignored_disk(item):
                return

            if name == 'Disk_Read_Bytes_persec':
                name = 'io_read_bytes'
            elif name == 'Disk_Write_Bytes_persec':
                name = 'io_write_bytes'
            elif name == 'Current_Disk_Queue_Length':
                name = 'io_in_progress'
            elif name == 'Disk_Reads_persec':
                name = 'io_reads'
            elif name == 'Disk_Writes_persec':
                name = 'io_writes'
            elif name == 'Percent_Disk_Time':
                name = 'io_utilization'
                self.core.emit_metric({
                    'measurement': 'io_time',
                    # io_time is a number of ms spent doing IO (per seconds)
                    # utilization is 100% when we spent 1000ms during one
                    # second
                    'value': value * 1000. / 100.,
                    'time': timestamp,
                    'item': item,
                })
            elif name == 'Percent_Disk_Read_Time':
                name = 'io_read_time'
                # Like io_time/io_utilization
                value = value * 1000. / 100.
            elif name == 'Percent_Disk_Write_Time':
                name = 'io_write_time'
                # Like io_time/io_utilization
                value = value * 1000. / 100.
            else:
                return
        elif part[-2] == 'mem':
            name = 'mem_' + part[-1]
            if name in ('mem_used', 'mem_used_percent'):
                # We don't use mem_used of telegraf (which is
                # mem_total - mem_free)
                # We prefere the "collectd one" (which is
                # mem_total - (mem_free + mem_cached + mem_buffered + mem_slab)

                # mem_used will be computed as mem_total - mem_available
                return  # We don't use mem_used of telegraf.
            elif name == 'mem_available_percent':
                name = 'mem_available_perc'
            elif name in ('mem_buffered', 'mem_cached', 'mem_free'):
                pass
            elif name in ('mem_total', 'mem_available'):
                self.computed_metrics_pending.add(
                    ('mem_used', None, None, timestamp)
                )
            else:
                return
        elif part[-2] == 'win_mem':
            name = part[-1]
            if name == 'Available_Bytes':
                name = 'mem_available'
                self.core.emit_metric({
                    'measurement': 'mem_available_perc',
                    'time': timestamp,
                    'value': value * 100. / self.core.total_memory_size,
                })
                mem_used = self.core.total_memory_size - value
                self.core.emit_metric({
                    'measurement': 'mem_used',
                    'time': timestamp,
                    'value': mem_used,
                })
                self.core.emit_metric({
                    'measurement': 'mem_used_perc',
                    'time': timestamp,
                    'value': mem_used * 100. / self.core.total_memory_size,
                })
                self.computed_metrics_pending.add(
                    ('mem_free', None, None, timestamp)
                )
            elif name in (
                    'Standby_Cache_Reserve_Bytes',
                    'Standby_Cache_Normal_Priority_Bytes',
                    'Standby_Cache_Core_Bytes'):
                no_emit = True
                self.computed_metrics_pending.add(
                    ('mem_cached', None, None, timestamp)
                )
            else:
                return
        elif part[-2] == 'net' and part[-3] != 'all':
            item = part[-3]
            if self.graphite_server.network_interface_blacklist(item):
                return

            name = 'net_' + part[-1]
            if name == 'net_bytes_recv' or name == 'net_bytes_sent':
                name = name.replace('bytes', 'bits')
                value = value * 8

            derive = True
        elif part[-2] == 'win_net':
            item = part[2]
            name = part[-1]
            if self.graphite_server.network_interface_blacklist(item):
                return

            if name == 'Bytes_Sent_persec':
                name = 'net_bits_sent'
                value = value * 8
            elif name == 'Bytes_Received_persec':
                name = 'net_bits_recv'
                value = value * 8
            elif name == 'Packets_Sent_persec':
                name = 'net_packets_sent'
            elif name == 'Packets_Received_persec':
                name = 'net_packets_recv'
            elif name == 'Packets_Received_Discarded':
                derive = True
                name = 'net_drop_in'
            elif name == 'Packets_Outbound_Discarded':
                derive = True
                name = 'net_drop_out'
            elif name == 'Packets_Received_Errors':
                derive = True
                name = 'net_err_in'
            elif name == 'Packets_Outbound_Errors':
                derive = True
                name = 'net_err_out'
            else:
                return
        elif part[-2] == 'swap':
            if not self.core.last_facts.get('swap_present', False):
                return
            name = 'swap_' + part[-1]
            if name.endswith('_percent'):
                name = name.replace('_percent', '_perc')
            if name in ('swap_in', 'swap_out'):
                derive = True
        elif part[-2] == 'win_swap':
            if not self.core.last_facts.get('swap_present', False):
                return

            name = part[-1]
            if name == 'Percent_Usage':
                name = 'swap_used_perc'
                if value == 0:
                    swap_used = 0.0
                else:
                    swap_used = self.core.total_swap_size / (value / 100.)
                self.core.emit_metric({
                    'measurement': 'swap_used',
                    'time': timestamp,
                    'value': swap_used,
                })
                self.core.emit_metric({
                    'measurement': 'swap_free',
                    'time': timestamp,
                    'value': self.core.total_swap_size - swap_used,
                })
        elif part[-2] == 'system':
            name = 'system_' + part[-1]
            if name == 'system_uptime':
                name = 'uptime'
            elif name == 'system_n_users':
                name = 'users_logged'
            elif name not in ('system_load1', 'system_load5', 'system_load15'):
                return
        elif part[-2] == 'win_system':
            name = part[-1]
            if name == 'System_Up_Time':
                name = 'uptime'
            elif name == 'Processor_Queue_Length':
                no_emit = True
                self.computed_metrics_pending.add(
                    ('system_load1', None, None, timestamp)
                )
            else:
                return
        elif part[-2] == 'processes':
            if part[-1] in ['blocked', 'running', 'sleeping',
                            'stopped', 'zombies', 'paging']:
                name = 'process_status_%s' % part[-1]
            elif part[-1] == 'total':
                name = 'process_total'
            else:
                return
        elif part[-2] == 'apache':
            service = 'apache'
            server_address = part[-3].replace('_', '.')
            server_port = int(part[-4])
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = 'apache_' + part[-1]
            if name == 'apache_IdleWorkers':
                name = 'apache_idle_workers'
            elif name == 'apache_TotalAccesses':
                name = 'apache_requests'
                derive = True
            elif name == 'apache_TotalkBytes':
                name = 'apache_bytes'
                derive = True
            elif name == 'apache_ConnsTotal':
                name = 'apache_connections'
            elif name == 'apache_Uptime':
                name = 'apache_uptime'
            elif 'scboard' in name:
                name = name.replace('scboard', 'scoreboard')
            else:
                return
        elif part[-2] == 'haproxy':
            service = 'haproxy'
            proxy_name = part[2]
            if part[4] not in ('BACKEND', 'FRONTEND'):
                return
            hostport = part[3].replace('_', '.')
            try:
                instance = self.get_haproxy_instance(hostport)
            except KeyError:
                return

            if (part[-1] in ('stot', 'bin', 'bout', 'dreq', 'dresp', 'ereq',
                             'econ', 'eresp', 'req_tot')):
                derive = True
                name = 'haproxy_' + part[-1]
            elif (part[-1] in ('qcur', 'scur', 'qtime', 'ctime', 'rtime',
                               'ttime')):
                name = 'haproxy_' + part[-1]
            elif part[-1] == 'active_servers':
                name = 'haproxy_act'
            else:
                return

            if instance is None:
                item = proxy_name
            else:
                item = instance + '_' + proxy_name
        elif part[-2] == 'memcached':
            service = 'memcached'
            (server_address, server_port) = part[-3].split(':')
            server_address = server_address.replace('_', '.')
            server_port = int(server_port)
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = 'memcached_' + part[-1]
            if '_cmd_' in name:
                name = name.replace('_cmd_', '_command_')
                derive = True
            elif name == 'memcached_curr_connections':
                name = 'memcached_connections_current'
            elif name == 'memcached_curr_items':
                name = 'memcached_items_current'
            elif name == 'memcached_bytes_read':
                name = 'memcached_octets_rx'
                derive = True
            elif name == 'memcached_bytes_written':
                name = 'memcached_octets_tx'
                derive = True
            elif name == 'memcached_evictions':
                name = 'memcached_ops_evictions'
                derive = True
            elif name == 'memcached_threads':
                name = 'memcached_ps_count_threads'
            elif name in ('memcached_get_misses', 'memcached_get_hits'):
                name = name.replace('_get_', '_ops_')
                derive = True
            elif name.endswith('_misses') or name.endswith('_hits'):
                name = name.replace('memcached_', 'memcached_ops_')
                derive = True
            elif name != 'memcached_uptime':
                return
        elif part[-2] in ('mysql', 'mysql_innodb'):
            service = 'mysql'
            (server_address, server_port) = part[-3].split(':')
            server_address = server_address.replace('_', '.')
            server_port = int(server_port)
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = part[-2] + '_' + part[-1]
            derive = True
            if name.startswith('mysql_qcache_'):
                name = name.replace('qcache', 'cache_result_qcache')
                if name == 'mysql_cache_result_qcache_lowmem_prunes':
                    name = 'mysql_cache_result_qcache_prunes'
                elif name == 'mysql_cache_result_qcache_queries_in_cache':
                    name = 'mysql_cache_size_qcache'
                    derive = False
                elif name == 'mysql_cache_result_qcache_total_blocks':
                    name = 'mysql_cache_blocksize_qcache'
                    derive = False
                elif (name == 'mysql_cache_result_qcache_free_blocks'
                        or name == 'mysql_cache_result_qcache_free_memory'):
                    name = name.replace(
                        'mysql_cache_result_qcache_', 'mysql_cache_')
                    derive = False
            elif name.startswith('mysql_table_locks_'):
                name = name.replace('mysql_table_locks_', 'mysql_locks_')
            elif name == 'mysql_bytes_received':
                name = 'mysql_octets_rx'
            elif name == 'mysql_bytes_sent':
                name = 'mysql_octets_tx'
            elif name == 'mysql_threads_created':
                name = 'mysql_total_threads_created'
            elif name.startswith('mysql_threads_'):
                # Other mysql_threads_* name are fine. Accept them unchanged
                derive = False
                pass
            elif name.startswith('mysql_commands_'):
                # mysql_commands_* name are fine. Accept them unchanged
                pass
            elif name.startswith('mysql_handler_'):
                # mysql_handler_* name are fine. Accept them unchanged
                pass
            elif name in ('mysql_queries', 'mysql_slow_queries'):
                pass
            elif name == 'mysql_innodb_row_lock_current_waits':
                derive = False
                name = 'mysql_innodb_locked_transaction'
            elif name == 'mysql_innodb_trx_rseg_history_len':
                derive = False
                name = 'mysql_innodb_history_list_len'
            else:
                return
        elif part[-2] == 'nginx':
            service = 'nginx'
            server_address = part[-3].replace('_', '.')
            server_port = int(part[-4])
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = 'nginx_connections_' + part[-1]
            if name == 'nginx_connections_requests':
                name = 'nginx_requests'
                derive = True
            elif name == 'nginx_connections_accepts':
                name = 'nginx_connections_accepted'
                derive = True
            elif name == 'nginx_connections_handled':
                derive = True
        elif part[-2] == 'postgresql':
            service = 'postgresql'
            dbname = part[2]

            if dbname in ('template0', 'template1'):
                return

            connect_string = part[3]
            # connect string look like:
            # "host=172_17_0_4_port=5432_user=bleemeo_user_dbname=postgres"
            match = re.match(
                r'^host=(.*)_port=(.*)_user=.*$',
                connect_string,
            )
            if not match:
                return

            server_address = match.group(1).replace('_', '.')
            server_port = int(match.group(2))
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            derive = True
            if part[-1] == 'xact_commit':
                name = 'postgresql_commit'
            elif part[-1] == 'xact_rollback':
                name = 'postgresql_rollback'
            elif (part[-1] in ('blks_read', 'blks_hit', 'tup_returned',
                               'tup_fetched', 'tup_inserted', 'tup_updated',
                               'tup_deleted', 'temp_files', 'temp_bytes',
                               'blk_read_time', 'blk_write_time')):
                name = 'postgresql_' + part[-1]
            else:
                return

            if instance is None:
                item = dbname
            else:
                item = instance + '_' + dbname
        elif part[-2] == 'redis':
            service = 'redis'

            # Prior to Telegraf 0.13.1, output was
            # telegraf.$HOSTNAME.$PORT.$SERVER.redis.$METRIC
            # Telegraf 0.13.1+, output is
            # telegraf.$HOSTNAME.$PORT.$ROLE.$SERVER.redis.$METRIC

            # Also, for both a $DATABASE may exists just after $HOSTNAME
            # E.g for 0.13.1:
            # telegraf.$HOSTNAME.$DATABASE.$PORT.$ROLE.$SERVER.redis.$METRIC
            #
            # $PORT is part[-4] or part[-5]
            # $SERVER is always part[-3]
            server_address = part[-3].replace('_', '.')
            if part[-4] in ('master', 'slave'):
                server_port = int(part[-5])
            else:
                server_port = int(part[-4])
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = 'redis_' + part[-1]

            if name == 'redis_clients':
                name = 'redis_current_connections_clients'
            elif name == 'redis_connected_slaves':
                name = 'redis_current_connections_slaves'
            elif name.startswith('redis_used_memory'):
                name = name.replace('redis_used_memory', 'redis_memory')
            elif name == 'redis_total_connections_received':
                name = 'redis_total_connections'
                derive = True
            elif name == 'redis_total_commands_processed':
                name = 'redis_total_operations'
                derive = True
            elif name == 'redis_rdb_changes_since_last_save':
                name = 'redis_volatile_changes'
            elif name in ('redis_evicted_keys', 'redis_keyspace_hits',
                          'redis_keyspace_misses', 'redis_expired_keys'):
                derive = True
            elif name in ('redis_uptime', 'redis_pubsub_patterns',
                          'redis_pubsub_channels', 'redis_keyspace_hitrate'):
                pass
            else:
                return
        elif part[-2] == 'zookeeper':
            service = 'zookeeper'

            # Telegraf 1.0.0 added "state" in tag. Which change position of
            # server_address and server_port.

            # Telegraf <1.0.0, output was:
            # telegraf.$HOSTNAME.$PORT.$SERVER.zookeeper.$METRIC
            # Telegraf 1.0.0+, output is:
            # telegraf.$HOSTNAME.$PORT.$SERVER.$STATE.zookeeper.$METRIC
            server_address = part[3].replace('_', '.')
            server_port = int(part[2])
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = 'zookeeper_' + part[-1]
            if name.startswith('zookeeper_packets_'):
                derive = True
            elif name in ('zookeeper_ephemerals_count',
                          'zookeeper_watch_count', 'zookeeper_znode_count'):
                pass
            elif name == 'zookeeper_num_alive_connections':
                name = 'zookeeper_connections'
            else:
                return
        elif part[-2] == 'mongodb':
            service = 'mongodb'
            (server_address, server_port) = part[-3].split(':')
            server_address = server_address.replace('_', '.')
            server_port = int(server_port)
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            name = 'mongodb_' + part[-1]
            if name in ('mongodb_open_connections', 'mongodb_queued_reads',
                        'mongodb_queued_writes', 'mongodb_active_reads',
                        'mongodb_active_writes'):
                pass
            elif name == 'mongodb_queries_per_sec':
                name = 'mongodb_queries'
            elif name in ('mongodb_net_out_bytes', 'mongodb_net_in_bytes'):
                derive = True
            else:
                return
        elif part[-2].startswith('elasticsearch_'):
            service = 'elasticsearch'
            # It can't rely only on part[3] (node_host), because this is the
            # host as think by ES (usually the "public" IP of the node). But
            # Agent use "127.0.0.1" for localhost.
            # server_address = part[3].replace('_', '.')
            node_id = part[4]
            try:
                instance = self.get_elasticsearch_instance(node_id)
            except KeyError:
                return

            name = None
            if part[-2] == 'elasticsearch_indices':
                if part[-1] == 'docs_count':
                    name = 'elasticsearch_docs_count'
                elif part[-1] == 'store_size_in_bytes':
                    name = 'elasticsearch_size'
                elif part[-1] == 'search_query_total':
                    name = 'elasticsearch_search'
                    derive = True
                    self.computed_metrics_pending.add(
                        (
                            'elasticsearch_search_time',
                            instance,
                            instance,
                            timestamp
                        )
                    )
                elif part[-1] == 'search_query_time_in_millis':
                    name = 'elasticsearch_search_time_total'
                    derive = True
                    no_emit = True
                    self.computed_metrics_pending.add(
                        (
                            'elasticsearch_search_time',
                            instance,
                            instance,
                            timestamp
                        )
                    )
            elif part[-2] == 'elasticsearch_jvm':
                if part[-1] == 'mem_heap_used_in_bytes':
                    name = 'elasticsearch_jvm_heap_used'
                elif part[-1] == 'mem_non_heap_used_in_bytes':
                    name = 'elasticsearch_jvm_non_heap_used'
                elif part[-1] == 'gc_collectors_old_collection_count':
                    name = 'elasticsearch_jvm_gc_old'
                    no_emit = True
                    derive = True
                    self.computed_metrics_pending.add(
                        (
                            'elasticsearch_jvm_gc',
                            instance,
                            instance,
                            timestamp
                        )
                    )
                elif part[-1] == 'gc_collectors_young_collection_count':
                    name = 'elasticsearch_jvm_gc_young'
                    no_emit = True
                    derive = True
                    self.computed_metrics_pending.add(
                        (
                            'elasticsearch_jvm_gc',
                            instance,
                            instance,
                            timestamp
                        )
                    )
                elif part[-1] == 'gc_collectors_old_collection_time_in_millis':
                    name = 'elasticsearch_jvm_gc_time_old'
                    no_emit = True
                    derive = True
                    self.computed_metrics_pending.add(
                        (
                            'elasticsearch_jvm_gc_time',
                            instance,
                            instance,
                            timestamp
                        )
                    )
                elif part[-1] == (
                        'gc_collectors_young_collection_time_in_millis'):
                    name = 'elasticsearch_jvm_gc_time_young'
                    no_emit = True
                    derive = True
                    self.computed_metrics_pending.add(
                        (
                            'elasticsearch_jvm_gc_time',
                            instance,
                            instance,
                            timestamp
                        )
                    )
        elif part[-2] == 'rabbitmq_overview':
            service = 'rabbitmq'

            tmp = part[-3]
            if not tmp.startswith('http:--'):
                return  # unknown format
            tmp = tmp[len('http:--'):]
            (server_address, server_port) = tmp.split(':')
            server_address = server_address.replace('_', '.')
            server_port = int(server_port)
            try:
                instance = self.get_service_instance(
                    service, server_address, server_port
                )
            except KeyError:
                return

            if part[-1] == 'messages':
                name = 'rabbitmq_messages_count'
            elif part[-1] == 'consumers':
                name = 'rabbitmq_consumers'
            elif part[-1] == 'connections':
                name = 'rabbitmq_connections'
            elif part[-1] == 'queues':
                name = 'rabbitmq_queues'
            elif part[-1] == 'messages_published':
                derive = True
                name = 'rabbitmq_messages_published'
            elif part[-1] == 'messages_delivered':
                derive = True
                name = 'rabbitmq_messages_delivered'
            elif part[-1] == 'messages_acked':
                derive = True
                name = 'rabbitmq_messages_acked'
            elif part[-1] == 'messages_unacked':
                name = 'rabbitmq_messages_unacked_count'
            else:
                return
        elif part[-2] == 'docker':
            if part[-1] == 'n_containers':
                name = 'docker_containers'
            else:
                return
        elif part[-2] == 'docker_container_cpu':
            if part[-1] == 'usage_total':
                name = 'docker_container_cpu_used'
                # Docker sends time in nanosecond. Convert it to seconds
                value = value / 1000000000

                # And return a percentage
                derive = True
                value = value * 100
            else:
                return

            container_name = self.docker_container_name(part)
            if container_name is None:
                return
            item = container_name

            # Only send metric for cpu=cpu-total
            # cpu tag is normally at part[5] position. But any label
            # before "cpu" will change its place
            inspect = self.core.docker_containers[container_name]
            labels = inspect.get('Config', {}).get('Labels', {})
            if labels is None:
                labels = {}
            label_keys_before = [
                key for (key, value) in labels.items()
                if key < 'cpu' and value != ''
            ]
            position = 5 + len(label_keys_before)
            if len(part) <= position or part[position] != 'cpu-total':
                return
        elif part[-2] == 'docker_container_mem':
            if part[-1] == 'usage_percent':
                name = 'docker_container_mem_used_perc'
            elif part[-1] == 'usage':
                name = 'docker_container_mem_used'
            else:
                return

            container_name = self.docker_container_name(part)
            if container_name is None:
                return
            item = container_name
        elif part[-2] == 'docker_container_net':
            if part[-1] == 'rx_bytes':
                name = 'docker_container_net_bits_recv'
                value = value * 8  # Convert bytes => bits
                derive = True
            elif part[-1] == 'tx_bytes':
                name = 'docker_container_net_bits_sent'
                value = value * 8  # Convert bytes => bits
                derive = True
            else:
                return

            container_name = self.docker_container_name(part)
            if container_name is None:
                return
            item = container_name

            # Only send metric for network=total
            # network tag is normally at part[-3] position. But any label
            # after "network" will change its place
            # Note: unlike "device" (for blkio) or "container_name" (for
            # docker_container_name), here we use offset from the end & label
            # *AFTER*, because Telegraf 1.1.0 introduced a new "engine_host"
            # tag. For "device" and "container_name", "engine_host" is AFTER.
            # Here "engine_host" is BEFORE "network". Using this allow same
            # code for all version of Telegraf.
            inspect = self.core.docker_containers[container_name]
            labels = inspect.get('Config', {}).get('Labels', {})
            if labels is None:
                labels = {}
            label_keys_after = [
                key for (key, value) in labels.items()
                if key > 'network' and value != ''
            ]
            position = 3 + len(label_keys_after)
            if len(part) <= position or part[-position] != 'total':
                return
        elif part[-2] == 'docker_container_blkio':
            if part[-1] == 'io_service_bytes_recursive_read':
                name = 'docker_container_io_read_bytes'
                derive = True
            elif part[-1] == 'io_service_bytes_recursive_write':
                name = 'docker_container_io_write_bytes'
                derive = True
            else:
                return

            container_name = self.docker_container_name(part)
            if container_name is None:
                return
            item = container_name

            # Only send metric for device=total
            # device tag is normally at part[5] position. But any label
            # before "device" will change its place
            inspect = self.core.docker_containers[container_name]
            labels = inspect.get('Config', {}).get('Labels', {})
            if labels is None:
                labels = {}
            label_keys_before = [
                key for (key, value) in labels.items()
                if key < 'device' and value != ''
            ]
            position = 5 + len(label_keys_before)
            if len(part) <= position or part[position] != 'total':
                print(part)
                return
        elif part[-2].startswith('prometheus_'):
            name = part[-2][len('prometheus_'):]
            try:
                prometheus_name = self.get_prometheus_exporter_name(name, part)
            except KeyError:
                logging.debug(
                    'Unknown prometheus exporter.'
                    ' Is it configured in Bleemeo agent ?'
                    ' And telegraf restarted after last telegraf'
                    ' config update ?'
                )
                return
            exporter_config = (
                self.core.config.get('metric.prometheus')[prometheus_name]
            )

            # tags will contains:
            # * url
            # * all prometheus label
            # Remove host and url, keep other in item
            url_mangled = telegraf_replace(exporter_config['url'])
            item_part = []
            for x in part[2:-2]:
                if x != url_mangled:
                    item_part.append(x)
            item = '-'.join(item_part)
            if part[-1] == 'counter':
                if name.endswith('_total'):
                    # Agent don't send a total, but a derivate.
                    name = name[:-len('_total')]
                derive = True
            elif part[-1] == 'gauge':
                pass
            elif part[-1] == 'sum':
                derive = True
                no_emit = True
                self.computed_metrics_pending.add(
                    ('prometheus_' + name, item, None, timestamp)
                )
                name = name + '_sum'
            elif part[-1] == 'count':
                derive = True
                no_emit = True
                self.computed_metrics_pending.add(
                    ('prometheus_' + name, item, None, timestamp)
                )
                name = name + '_count'
            elif part[-1] in ('5', '0', '1'):
                # This is used by quantile and histogram. (0 is in
                # fact 0.1, 0.25...)
                # Agent don't process them on use only sum and count.
                return
            else:
                logging.debug(
                    'Unknown Prometheus metric: %s_%s', name, part[-1],
                )
                return
        elif part[-2] == 'phpfpm':
            service = 'phpfpm'
            if ('phpfpm', part[2]) in self.core.services:
                instance = part[2]

            name = 'phpfpm_' + part[-1]

            if name in {'phpfpm_accepted_conn', 'phpfpm_slow_requests'}:
                derive = True
        elif (part[2] == 'counter'
                and self.core.config.get('telegraf.statsd.enabled', True)):
            # statsd counter
            derive = True
            name = 'statsd_' + part[3]
        elif (part[2] == 'gauge'
                and self.core.config.get('telegraf.statsd.enabled', True)):
            # statsd gauge
            name = 'statsd_' + part[3]
        elif (part[2] == 'timing'
                and self.core.config.get('telegraf.statsd.enabled', True)):
            # statsd timing
            name = 'statsd_' + part[3] + '_' + part[4]
            if part[4] == 'count':
                # count for timing are number of item per 10 seconds.
                # We want a count per second.
                value = value / 10
        else:
            return

        if name is None:
            return

        if item is None and service is not None:
            item = instance

        if derive:
            value = self.get_derivate(name, item, timestamp, value)
            if value is None:
                return
        metric = {
            'measurement': name,
            'time': timestamp,
            'value': value,
        }
        if service is not None:
            metric['service'] = service
            metric['instance'] = instance
        if item is not None:
            metric['item'] = item
        if container_name is not None:
            metric['container'] = container_name

        self.core.emit_metric(metric, no_emit=no_emit)

    def packet_finish(self):
        """ Called when graphite_client finished processing one TCP packet
        """
        self._check_computed_metrics()

    def _check_computed_metrics(self):
        """ Some metric are computed from other one. For example CPU stats
            are aggregated over all CPUs.

            When any cpu state arrive, we flag the aggregate value as "pending"
            and this function check if stats for all CPU core are fresh enough
            to compute the aggregate.

            This function use computed_metrics_pending, which old a list
            of (metric_name, item, timestamp).
            Item is something like "sda", "sdb" or "eth0", "eth1".
        """
        processed = set()
        new_item = set()
        for entry in self.computed_metrics_pending:
            (name, item, instance, timestamp) = entry
            try:
                self._compute_metric(name, item, instance, timestamp, new_item)
                processed.add(entry)
            except ComputationFail:
                logging.debug(
                    'Failed to compute metric %s at time %s',
                    name, timestamp)
                # we will never be able to recompute it.
                # mark it as done and continue :/
                processed.add(entry)
            except MissingMetric:
                # Some metric are missing to do computing. Wait a bit by
                # keeping this entry in computed_metrics_pending
                pass

        self.computed_metrics_pending.difference_update(processed)
        if new_item:
            self.computed_metrics_pending.update(new_item)
            self._check_computed_metrics()

    def _compute_metric(self, name, item, instance, timestamp, new_item):  # NOQA
        def get_metric(measurements, searched_item):
            """ Helper that do common task when retriving metrics:

                * check that metric exists and is not too old
                  (or Raise MissingMetric)
                * If the last metric is more recent that the one we want
                  to compute, raise ComputationFail. We will never be
                  able to compute the requested value.
            """
            metric = self.core.get_last_metric(measurements, searched_item)
            if metric is None or metric['time'] < timestamp:
                raise MissingMetric()
            elif metric['time'] > timestamp:
                raise ComputationFail()
            return metric['value']

        service = None

        if name == 'disk_total' and os.name == 'nt':
            used_perc = get_metric('disk_used_perc', item)
            free = get_metric('disk_free', item)
            free_perc = 100 - used_perc
            disk_total = free / (free_perc / 100.0)
            disk_used = disk_total * (used_perc / 100.0)
            value = disk_total
            self.core.emit_metric({
                'measurement': 'disk_used',
                'time': timestamp,
                'item': item,
                'value': disk_used,
            })
        elif name == 'cpu_other':
            value = get_metric('cpu_used', None)
            value -= get_metric('cpu_user', None)
            value -= get_metric('cpu_system', None)
        elif name == 'mem_free' and os.name == 'nt':
            used = get_metric('mem_used', item)
            cached = get_metric('mem_cached', item)
            value = self.core.total_memory_size - used - cached
        elif name == 'mem_cached' and os.name == 'nt':
            value = 0
            for sub_type in (
                    'Standby_Cache_Reserve_Bytes',
                    'Standby_Cache_Normal_Priority_Bytes',
                    'Standby_Cache_Core_Bytes'):
                value += get_metric(sub_type, item)
            new_item.add(
                ('mem_free', None, None, timestamp)
            )
        elif name == 'mem_used':
            total = get_metric('mem_total', None)
            value = total - get_metric('mem_available', None)
            self.core.emit_metric({
                'measurement': 'mem_used_perc',
                'time': timestamp,
                'value': value / total * 100.,
            })
        elif name == 'system_load1':
            # Unix load represent the number of running and runnable tasks
            # which include task waiting for disk IO.

            # To re-create the value on Windows:
            # Number of running task will be number of CPU core * CPU usage
            # Number of runnable tasks will be "Processor Queue Length", but
            # this probably does not include task waiting for disk IO.

            cpu_used = get_metric('cpu_used', None)
            runq = get_metric('Processor_Queue_Length', None)
            core_count = psutil.cpu_count()
            if core_count is None:
                core_count = 1
            value = core_count * (cpu_used / 100.) + runq
        elif name == 'elasticsearch_search_time':
            service = 'elasticsearch'
            total = get_metric('elasticsearch_search_time_total', item)
            count = get_metric('elasticsearch_search', item)
            if count == 0:
                # If not query during the period, the average time
                # has not meaning. Don't emit it at all.
                return
            value = total / count
        elif name == 'elasticsearch_jvm_gc':
            service = 'elasticsearch'
            gc_old = get_metric('elasticsearch_jvm_gc_old', item)
            gc_young = get_metric('elasticsearch_jvm_gc_young', item)
            value = gc_old + gc_young
        elif name == 'elasticsearch_jvm_gc_time':
            service = 'elasticsearch'
            gc_old = get_metric('elasticsearch_jvm_gc_time_old', item)
            gc_young = get_metric('elasticsearch_jvm_gc_time_young', item)
            value = gc_old + gc_young

            metric = {
                'measurement': 'elasticsearch_jvm_gc_utilization',
                'time': timestamp,
                'service': service,
                'value': value / 10.,  # convert ms/s in %
            }
            if item is not None:
                metric['item'] = item

            self.core.emit_metric(metric)
        elif name.startswith('prometheus_'):
            name = name[len('prometheus_'):]
            count = get_metric(name + '_count', item)
            total = get_metric(name + '_sum', item)
            if count == 0:
                # If no item during the period, the average has
                # no meaning. Don't emit the metric at all.
                return
            value = total / count
        else:
            logging.debug('Unknown computed metric %s', name)
            return

        metric = {
            'measurement': name,
            'time': timestamp,
            'value': value,
        }
        if item is not None:
            metric['item'] = item
        if service is not None:
            metric['service'] = service
            metric['instance'] = instance
        self.core.emit_metric(metric)


def _get_telegraf_config(core):
    telegraf_config = BASE_TELEGRAF_CONFIG

    if core.config.get('telegraf.statsd.enabled', True):
        telegraf_config += STATSD_TELEGRAF_CONFIG % {
            'address': core.config.get(
                'telegraf.statsd.address', '127.0.0.1',
            ),
            'port': core.config.get(
                'telegraf.statsd.port', '8125',
            ),
        }

    if core.docker_client is not None:
        docker_metrics_enabled = core.config.get(
            'telegraf.docker_metrics_enabled', None
        )
        if (docker_metrics_enabled
                or (
                    docker_metrics_enabled is None
                    and telegraf_version_gte(core, '1.0.0')
                )):
            telegraf_config += DOCKER_TELEGRAF_CONFIG

    for (key, service_info) in services_sorted(core.services.items()):
        if not service_info.get('active', True):
            continue
        (service_name, instance) = key

        service_info = core.services[key].copy()
        service_info['instance'] = instance

        if service_info.get('address') is None and instance is not None:
            # Address is None if this check is associated with a stopped
            # container. In such case, no metrics could be gathered.
            continue

        if (service_name == 'haproxy'
                and service_info.get('stats_url') is not None):
            telegraf_config += HAPROXY_TELEGRAF_CONFIG % service_info
        if (service_name == 'phpfpm'
                and (
                    service_info.get('stats_url') is not None
                    or service_info.get('port') is not None
                    or len(service_info.get('netstat_ports')) > 0)):
            copy_info = service_info.copy()
            port = service_info.get('port')
            for port_proto in service_info.get('netstat_ports', {}):
                if port is not None:
                    break
                if port_proto.endswith('/tcp'):
                    port = port_proto.split('/')[0]
            if ('stats_url' not in copy_info
                    and service_info.get('address') is None):
                continue

            copy_info.setdefault(
                'stats_url',
                'fcgi://%s:%s/status' % (
                    service_info.get('address'),
                    port,
                )
            )
            if instance:
                copy_info['tags'] = (
                    '[inputs.phpfpm.tags]\n        instance = "%s"' %
                    instance
                )
            else:
                copy_info['tags'] = ""
            telegraf_config += PHPFPM_TELEGRAF_CONFIG % copy_info

        # All next services require an address.
        if service_info.get('address') is None:
            continue

        if service_name == 'rabbitmq':
            service_info.setdefault('username', 'guest')
            service_info.setdefault('password', 'guest')
            service_info.setdefault('mgmt_port', 15672)
            telegraf_config += RABBITMQ_TELEGRAF_CONFIG % service_info

        # All next services require a port.
        if service_info.get('port') is None:
            continue

        if service_name == 'apache':
            if service_info.get('port') == 80:
                status_url = 'http://%(address)s/server-status?auto'
            else:
                status_url = 'http://%(address)s:%(port)s/server-status?auto'
            service_info = service_info.copy()
            service_info.setdefault('status_url', status_url % service_info)
            telegraf_config += APACHE_TELEGRAF_CONFIG % service_info
            if telegraf_version_gte(core, '1.2.0'):
                telegraf_config += APACHE_TELEGRAF_CONFIG_1_2
        if service_name == 'elasticsearch':
            telegraf_config += ELASTICSEARCH_TELEGRAF_CONFIG % service_info
        if service_name == 'memcached':
            telegraf_config += MEMCACHED_TELEGRAF_CONFIG % service_info
        if (service_name == 'mysql'
                and service_info.get('password') is not None):
            service_info.setdefault('username', 'root')
            telegraf_config += MYSQL_TELEGRAF_CONFIG % service_info
            if telegraf_version_gte(core, '1.3.0'):
                telegraf_config += MYSQL_TELEGRAF_CONFIG_1_3
        if service_name == 'mongodb':
            telegraf_config += MONGODB_TELEGRAF_CONFIG % service_info
        if service_name == 'nginx':
            telegraf_config += NGINX_TELEGRAF_CONFIG % service_info
            if telegraf_version_gte(core, '1.4.0'):
                telegraf_config += NGINX_TELEGRAF_CONFIG_1_4
        if (service_name == 'postgresql'
                and service_info.get('password') is not None):
            service_info.setdefault('username', 'postgres')
            telegraf_config += POSTGRESQL_TELEGRAF_CONFIG % service_info
        if service_name == 'redis':
            telegraf_config += REDIS_TELEGRAF_CONFIG % service_info
        if service_name == 'zookeeper':
            telegraf_config += ZOOKEEPER_CONFIG % service_info

    prometheus_config = core.config.get('metric.prometheus', {})
    for name in sorted(prometheus_config):
        exporter_config = prometheus_config[name]
        telegraf_config += PROMETHEUS_TELEGRAF_CONFIG % {
            'url': exporter_config['url'],
            'prefix': name,
        }

    return telegraf_config


def _restart_telegraf(core):
    restart_cmd = core.config.get(
        'telegraf.restart_command',
        'sudo -n service telegraf restart')
    telegraf_container = core.config.get('telegraf.docker_name')
    if telegraf_container is not None:
        bleemeo_agent.util.docker_restart(
            core.docker_client, telegraf_container
        )
    else:
        try:
            output = subprocess.check_output(
                shlex.split(restart_cmd),
                stderr=subprocess.STDOUT,
            )
            return_code = 0
        except (subprocess.CalledProcessError, OSError) as exception:
            output = exception.output
            return_code = exception.returncode

        if return_code != 0:
            logging.info(
                'Failed to restart telegraf after reconfiguration: %s',
                output
            )
        else:
            logging.debug(
                'telegraf reconfigured and restarted: %s', output)


def telegraf_version_gte(core, version):
    """ Return True if installed Telegraf version is at least given version

        If unable to compare given version with current version, return
        False (for example fact telegraf_version is absent or any error).
    """
    current_version = core.last_facts.get('telegraf_version')
    if current_version is None:
        return False

    try:
        return compare_version(current_version, version)
    except Exception:
        return False


def _write_config(core):
    telegraf_config = _get_telegraf_config(core)

    telegraf_config_path = core.config.get(
        'telegraf.config_file',
        '/etc/telegraf/telegraf.d/bleemeo-generated.conf'
    )

    if os.path.exists(telegraf_config_path):
        with open(telegraf_config_path) as fd:
            current_content = fd.read()

        if telegraf_config == current_content:
            logging.debug('telegraf already configured')
            return

    if (telegraf_config == BASE_TELEGRAF_CONFIG
            and not os.path.exists(telegraf_config_path)):
        logging.debug(
            'telegraf generated config would be empty, skip writting it'
        )
        return

    # Don't simply use open. This file must have limited permission
    # since it may contains password
    open_flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC
    fileno = os.open(telegraf_config_path, open_flags, 0o600)
    with os.fdopen(fileno, 'w') as fd:
        fd.write(telegraf_config)

    _restart_telegraf(core)
